{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a45a3d21",
   "metadata": {},
   "source": [
    "Checkout this [blog](https://aws.amazon.com/blogs/machine-learning/choose-the-best-data-source-for-your-amazon-sagemaker-training-job/) to verify if FSx is needed for your use-case to save op costs. This sample shows how to \n",
    "\n",
    "- Setup FSx\n",
    "- Mount data from S3\n",
    "- Run a SageMaker Training job using data from FSx mount\n",
    "- Save artifacts into FSx which are automatically pushed to S3\n",
    "- Tear down the infra\n",
    "\n",
    "\n",
    "**Please make sure the CIDR block in setup/cfn-nlp.yaml does not conflict with your existing VPC. You can change FSx storage (currently set at 1.2 TB) depending on your data sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d192a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs\n",
    "cfn_stack_name = 'large-scale-training' # cloudformation stack name\n",
    "s3_data_bucket = 's3://nlp-largescale-training' #your training data set, assumes training data is under the prefix train (s3://nlp-largescale-training/train/)\n",
    "fsx_file_system_path = 'gpt2' #this is file system path on FSx for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492eecef",
   "metadata": {},
   "source": [
    "Infra setup\n",
    "- Setup Networking Components and FSx\n",
    "- Configure FSx and add association to load data from S3\n",
    "\n",
    "**Please make sure the region you want to use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15964950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup infra stack for FSx\n",
    "!AWS_REGION=us-west-2 AWS_REGION_AZ=us-west-2c sh ./setup/stack-nlp.sh $cfn_stack_name\n",
    "\n",
    "# Grab security grp, fsx id and private subnet from the output of CFN\n",
    "tmp = !aws cloudformation describe-stacks --stack-name $cfn_stack_name --query \"Stacks[0].Outputs[?OutputKey=='sg' || OutputKey=='privatesubnet' || OutputKey=='outputfsx'].OutputValue\" --no-paginate --output text\n",
    "security_grp, fsx_id, private_subnet_id = tuple(str(tmp.s).split('\\t'))\n",
    "\n",
    "# Grab the fsx mount name\n",
    "tmp = !aws fsx describe-file-systems --file-system-ids $fsx_id --no-paginate --query \"FileSystems[0].LustreConfiguration.MountName\" --output text\n",
    "fsx_mount_name = str(tmp.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c59c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FSx to load data from S3 and persist changes back to s3 to save training artifacts (model, checkpoints)\n",
    "fsx_data_assoc_cmd = f'create-data-repository-association --file-system-id {fsx_id} --file-system-path /{fsx_file_system_path} --data-repository-path {s3_data_bucket} \\\n",
    "                        --batch-import-meta-data-on-create --s3 \"AutoExportPolicy={{Events=[NEW,CHANGED,DELETED]}}\"'\n",
    "\n",
    "!aws fsx $fsx_data_assoc_cmd\n",
    "\n",
    "# make sure the association is created and available\n",
    "def get_association_status(fsx_id):\n",
    "    tmp = !aws fsx describe-data-repository-associations --no-paginate --filters \"Name=file-system-id,Values={fsx_id}\" --query Associations[0].Lifecycle\n",
    "    status = str(tmp.s)\n",
    "    return status\n",
    "\n",
    "def wait_for_assoc_complete(fsx_id):\n",
    "    import time\n",
    "    status = get_association_status(fsx_id)\n",
    "    while status == '\"CREATING\"':\n",
    "        print(f'Waiting for s3 association in FSx, current status {status}  ...')\n",
    "        time.sleep(20)\n",
    "        status = get_association_status(fsx_id)\n",
    "    if status != '\"AVAILABLE\"':\n",
    "        raise SystemExit(f'Failed to create s3 associations in FSx, failure reason : {status}')\n",
    "    print(f'Association {status}.')\n",
    "    \n",
    "wait_for_assoc_complete(fsx_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db72ac8",
   "metadata": {},
   "source": [
    "Sample SageMaker Training job to showcase the parameters needed to be passed for FSx [Integration](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a267fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "# setup fsx config for data channels\n",
    "fsx_directory_path = f'/{fsx_mount_name}/{fsx_file_system_path}'\n",
    "fsx_input = FileSystemInput(\n",
    "    file_system_id=fsx_id,\n",
    "    file_system_type='FSxLustre',\n",
    "    directory_path=fsx_directory_path,\n",
    "    file_system_access_mode=\"rw\", # write needed for saving model artifacts to fsx\n",
    ")\n",
    "data_channels = {\"train\": fsx_input}\n",
    "\n",
    "# for ease, so that you can use fsx for data and training artifacts\n",
    "SM_TRAIN_DIR = \"/opt/ml/input/data/train\" #path where fsx is mounted in the training container\n",
    "hyperparameters = {}\n",
    "hyperparameters[\"checkpoint-dir\"] = f\"{SM_TRAIN_DIR}/checkpointdir\"\n",
    "hyperparameters[\"model-dir\"] = f\"{SM_TRAIN_DIR}/modeldir\"\n",
    "hyperparameters[\"training-dir\"] = f\"{SM_TRAIN_DIR}/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccfdf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup estimator and invoke\n",
    "instance_type = \"ml.p3.2xlarge\"\n",
    "instance_count = 1\n",
    "base_job_name = f'sagemaker-fsx-mount-sample'\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=os.getcwd(),\n",
    "    instance_type=instance_type,\n",
    "    role=get_execution_role(),\n",
    "    instance_count=instance_count,\n",
    "    framework_version=\"1.8.1\",\n",
    "    py_version=\"py36\",\n",
    "    checkpoint_s3_uri=None, #as it is FSx\n",
    "    checkpoint_local_path=hyperparameters[\"checkpoint-dir\"], #FSx\n",
    "    hyperparameters=hyperparameters,\n",
    "    base_job_name=base_job_name,\n",
    "    subnets = [private_subnet_id], # Give SageMaker Training Jobs access to FSx resources in your Amazon VPC\n",
    "    security_group_ids=[security_grp],\n",
    "    max_retry_attempts=30)\n",
    "\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9e1a72",
   "metadata": {},
   "source": [
    "Tear down infra, Training artifacts are uploaded to S3 from FSx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a85fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the stack\n",
    "!aws cloudformation delete-stack --stack-name $cfn_stack_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc54eb0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
